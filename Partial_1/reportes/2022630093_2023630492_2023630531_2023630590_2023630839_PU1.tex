\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{geometry}
\usepackage{graphicx}
% Paquete SVG removido - usando PNG en su lugar
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{float}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\usepackage[hidelinks]{hyperref}

% Configuracion de pagina
\geometry{margin=2.5cm}
\setlength{\headheight}{15pt}
\addtolength{\topmargin}{-2.5pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Procesamiento de Lenguaje Natural}
\fancyhead[R]{Primer Parcial - Practicas}
\fancyfoot[C]{\thepage}

% Configuracion de codigo
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    captionpos=b
}

% Configuracion para C++
\lstdefinestyle{cpp}{
    language=C++,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

\begin{document}

% Carátula
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\huge\bfseries Instituto Politécnico Nacional}\\[0.5cm]
    {\Large Escuela Superior de Cómputo}\\[1.5cm]
    
    % \includegraphics[width=0.3\textwidth]{ipn_logo.png}\\[1cm]
    {\Large INSTITUTO POLITÉCNICO NACIONAL}\\[1cm]
    
    {\huge\bfseries Procesamiento de Lenguaje Natural}\\[0.5cm]
    {\Large Primer Parcial - Prácticas}\\[2cm]
    
    {\Large\bfseries Reporte de Prácticas}\\[0.5cm]
    {\large Tokenizacion, Preprocesamiento y TF-IDF}\\[2cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft}
            \textbf{Alumnos:}\\
            De La Cruz Carmona Fernando Daniel\\[0.3cm]
            Mendez Carranza Edmundo Ramon\\[0.3cm]
            Rosas Sandoval Gustavo Issac\\[0.3cm]
            Sanchez Garcia Miguel Alexander\\[0.3cm]
            Villagran Salazar Diego\\[0.5cm]
\textbf{Grupo:} 6AV1\\
            \textbf{Carrera:} Licenciatura en Ciencia de Datos
        \end{flushleft}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\textwidth}
        \begin{flushright}
            \textbf{Profesor:}\\
            Ortiz Castillo Marco Antonio\\[0.5cm]
            \textbf{Fecha:}\\
            30 de Septiembre, 2024
        \end{flushright}
    \end{minipage}
    
    \vfill
\end{titlepage}

% Índice
\tableofcontents
\newpage

% Introducción General
\section{Introducción General}

El Procesamiento de Lenguaje Natural (PLN) es una rama de la inteligencia artificial que se enfoca en la interacción entre las computadoras y el lenguaje humano. En este reporte se presentan tres prácticas fundamentales que constituyen la base del procesamiento de texto:

\begin{enumerate}
    \item \textbf{Tokenizacion}: Proceso de dividir un texto en unidades mas pequenas llamadas tokens.
    \item \textbf{Preprocesamiento}: Limpieza y normalizacion del texto mediante la eliminacion de stopwords y conversion a minusculas.
    \item \textbf{TF-IDF}: Calculo de la importancia de terminos en una coleccion de documentos.
\end{enumerate}

Estas tecnicas son esenciales para cualquier sistema de PLN y forman la base para tareas mas complejas como analisis de sentimientos, clasificacion de texto y recuperacion de informacion.

\newpage

% Practica 1: Tokenizacion
\section{Practica 1: Tokenizacion}

\subsection{Introduccion}

La tokenizacion es el proceso fundamental de dividir un texto en unidades mas pequenas llamadas tokens. Estos tokens pueden ser palabras, numeros, simbolos o cualquier secuencia de caracteres que tenga significado en el contexto del analisis. En esta practica se implementa un tokenizador que:

\begin{itemize}
    \item Separa palabras usando delimitadores predefinidos
    \item Filtra numeros puros de palabras alfanumericas
    \item Mantiene solo caracteres alfabeticos en palabras mixtas
    \item Preserva numeros completos cuando aparecen solos
\end{itemize}

\subsection{Diagrama de Flujo}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/diagrama1.jpg}
    \caption{Diagrama de flujo del proceso de tokenización}
    \label{fig:diagrama_tokenizacion}
\end{figure}

\subsection{Código Fuente}

\subsubsection{Implementación en Python}

\begin{lstlisting}[caption=Clase Tokenizer en Python]
import time
import tracemalloc

class Tokenizer:
    """ Class for tokenizing text """
    delimiter = ""
    
    """ Constructor """
    def __init__(self):
        self.delimiter = " \t\n\r\f\v" + "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}"

    """ Methods """
    # Verifies if the word is only numbers or alphanumeric
    def verify_word(self, text:str) -> str:
        numbers = "0123456789"
        is_only_number = True
        word = ""
        for char in text:
            if char not in numbers:
                is_only_number = False
                break 

        if is_only_number:
            word = text
        else:
            # Keep alphabetic characters, remove only numbers from mixed words
            for char in text:
                if char.isalpha():  # Keep letters
                    word += char
        return word
    
    # Tokenizes the input text
    def tokenize(self, text: str) -> list:              
        t_init = time.time()
        tracemalloc.start()
        
        token = []
        n = len(text)
        
        i = 0
        j = i
        
        while i <= n - 1:
            if (text[i] in self.delimiter) and (text[j] in self.delimiter):
                j += 1
            elif (text[i] in self.delimiter):
                word_verified = self.verify_word(text[j:i])
                if word_verified:  # Only add non-empty words
                    token.append(word_verified)
                j = i + 1
            i += 1

        # Handle the last word if the text doesn't end with a delimiter
        if j < n:
            word_verified = self.verify_word(text[j:n])
            if word_verified:
                token.append(word_verified)

        tracemalloc.stop()
        
        return token
\end{lstlisting}

\subsubsection{Implementación en C++}

\begin{lstlisting}[style=cpp, caption=Clase Tokenizer en C++]
#include <string>
#include <vector>
#include <iostream>
#include <chrono>
#include <cstring>

using namespace std;
using namespace std::chrono;

class Tokenizer {
private:
    string delimiter;

public:
    Tokenizer() {
        delimiter = " \t\n\r\f\v!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~";
    }
    
    string verify_word(const string& text) {
        string numbers = "0123456789";
        bool is_only_number = true;
        string word = "";
        
        for (char c : text) {
            if (numbers.find(c) == string::npos) {
                is_only_number = false;
                break;
            }
        }
        
        if (is_only_number) {
            word = text;
        } else {
            for (char c : text) {
                if (numbers.find(c) == string::npos) {
                    word += c;
                }
            }
        }
        
        return word;
    }

    vector<string> tokenize(const string& text) {
        auto start = high_resolution_clock::now();
        
        vector<string> tokens;
        int n = text.length();
        
        int i = 0;
        int j = 0;
        
        while (i <= n - 1) {
            if ((delimiter.find(text[i]) != string::npos) && 
                (delimiter.find(text[j]) != string::npos)) {
                j++;
            } else if (delimiter.find(text[i]) != string::npos) {
                if (i > j) {  
                    string word_verified = verify_word(text.substr(j, i - j));
                    if (!word_verified.empty()) {  
                        tokens.push_back(word_verified);
                    }
                }
                j = i + 1;
            }
            i++;
        }
        
        if (j < n) {
            string word_verified = verify_word(text.substr(j));
            if (!word_verified.empty()) {
                tokens.push_back(word_verified);
            }
        }
        
        auto end = high_resolution_clock::now();
        auto duration = duration_cast<microseconds>(end - start);
        
        cout << "Time: " << duration.count() << " microseconds" << endl;

        return tokens;
    }
};
\end{lstlisting}

\subsection{Capturas del Funcionamiento}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_1.png}
        \caption{Funcionamiento del tokenizador en Python}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_1_1.jpeg}
        \caption{Funcionamiento del tokenizador en C++}
    \end{subfigure}
    \caption{Capturas adicionales del funcionamiento del primer ejercicio}
\end{figure}

\newpage

% Practica 2: Preprocesamiento
\section{Practica 2: Preprocesamiento de Texto, Lectura de PDF y One Hot Encoding}

\subsection{Introduccion}

El preprocesamiento de texto es una etapa crucial que mejora la calidad de los datos antes del analisis. En esta practica se extiende el tokenizador basico para incluir:

\begin{itemize}
    \item \textbf{Conversion a minusculas}: Normaliza el texto para evitar duplicados por diferencias de capitalizacion
    \item \textbf{Eliminacion de stopwords}: Remueve palabras comunes que no aportan significado semantico
    \item \textbf{Filtrado de contenido}: Mantiene solo palabras relevantes para el analisis
    \item \textbf{Lectura de archivos PDF}: Extraccion de texto desde documentos PDF
    \item \textbf{One Hot Encoding}: Creacion de matrices de representacion binaria del vocabulario
\end{itemize}

Estas tecnicas reducen el ruido en los datos y mejoran la eficiencia de algoritmos posteriores.

\subsection{Diagrama de Flujo}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/diagrama2.png}
    \caption{Diagrama de flujo del preprocesamiento de texto}
    \label{fig:diagrama_preprocesamiento}
\end{figure}

\subsection{Código Fuente}
\begin{lstlisting}[caption=Tokenizer con preprocesamiento]
import time
import tracemalloc

class Tokenizer:
    """ Class for tokenizing text """
    delimiter = ""
    
    """ Constructor """
    def __init__(self):
        self.delimiter = " \t\n\r\f\v" + "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"

    """ Methods """
    # Verifies if the word is only numbers or alphanumeric
    def verify_word(self, text:str) -> str:
        numbers = "0123456789"
        is_only_number = True
        word = ""
        for char in text:
            if char not in numbers:
                is_only_number = False
                break 

        if is_only_number:
            word = text
        else:
            # Keep alphabetic characters, remove only numbers from mixed words
            for char in text:
                if char.isalpha():  # Keep letters
                    word += char
        return word
    
    # Converts all characters in the token to lowercase
    def to_lowercase(self, token:list) -> list:
        for i in range(len(token)):
            for c in token[i]:
                if (c >= 'A') and (c <= 'Z'):
                    token[i] = token[i].replace(c, chr(ord(c) + 32))
        return token
    
    # Delete stopwords from the token
    def remove_stopwords(self, token:list) -> list:
        stopwords = ['the', 'of', 'in', 'on', 'a', 'an', 'some', 'and', 'that', 'this', 'mi', 'es', 'a', 'lo', 'la', 'el']
        return [word for word in token if word not in stopwords]
        
    def tokenize(self, text: str) -> list:              
        t_init = time.time()
        tracemalloc.start()
        
        token = []
        n = len(text)
        
        i = 0
        j = i
        
        while i <= n - 1:
            if (text[i] in self.delimiter) and (text[j] in self.delimiter):
                j += 1
            elif (text[i] in self.delimiter):
                word_verified = self.verify_word(text[j:i])
                if word_verified:  # Only add non-empty words
                    token.append(word_verified)
                j = i + 1
            i += 1

        # Handle the last word if the text doesn't end with a delimiter
        if j < n:
            word_verified = self.verify_word(text[j:n])
            if word_verified:
                token.append(word_verified)

        token = self.to_lowercase(token)
        token = self.remove_stopwords(token)

        tracemalloc.stop()
        
        return token
\end{lstlisting}

\subsection{Lectura de Archivo PDF y Aplicacion del Tokenizador}

\subsubsection{Codigo para Lectura de PDF}

\begin{lstlisting}[caption=Extracción de texto desde PDF]
import fitz

# Get the text from a PDF file
doc = fitz.open("el principito.pdf")

# Extract text from each page since third page
text = "\n".join([page.get_text() for page in doc[2:]])  

# Print the first 100 characters of the extracted text
print(text[:100])
\end{lstlisting}

\subsubsection{Tokenizacion del Documento Extraido}

\begin{lstlisting}[caption=Tokenización del texto extraído]
tokenizer = Tokenizer()

token_text = tokenizer.tokenize(text)
print(len(token_text)) 
print(token_text[:100])

# Get the unique words from the tokenized text
unique_words = set(token_text)
print(len(unique_words))
print(unique_words)
\end{lstlisting}

\subsection{One Hot Encoding}

\subsubsection{Implementacion de la Clase OneHotEncoder}

\begin{lstlisting}[caption=Clase OneHotEncoder]
import pandas as pd

class OneHotEncoder:
    """ Class for One Hot Encoding """
    def __init__(self):
        pass
    
    def fit_transform(self, token:list) -> dict:
        unique_words = set(token)
        # Order the set alphabetically
        unique_words = sorted(unique_words)
        one_hot_df = pd.DataFrame(0, index=range(len(unique_words)), 
                                   columns=list(unique_words))
        for i, word in enumerate(unique_words):
            one_hot_df.at[i, word] = 1
        return one_hot_df
\end{lstlisting}

\subsubsection{Aplicacion del One Hot Encoding}

\begin{lstlisting}[caption=Generación de la matriz One Hot]
oh_encoder = OneHotEncoder()

one_hot_token = oh_encoder.fit_transform(token_text)

one_hot_token
\end{lstlisting}

\newpage

\subsection{Descripcion del Proceso de One Hot Encoding}

El proceso de One Hot Encoding transforma cada palabra unica del vocabulario en un vector binario donde:

\begin{itemize}
    \item Cada fila representa una palabra unica del vocabulario
    \item Cada columna corresponde a una palabra del vocabulario ordenado alfabeticamente
    \item El valor 1 aparece en la posicion correspondiente a la palabra
    \item Todos los demas valores son 0
\end{itemize}

Esta representacion es util para algoritmos de machine learning que requieren entrada numerica, aunque puede ser ineficiente en terminos de memoria para vocabularios grandes.

\subsection{Capturas del Funcionamiento}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_2.png}
        \caption{Funcionamiento del preprocesamiento}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_2_1.png}
        \caption{Resultado del preprocesamiento}
    \end{subfigure}
    \caption{Capturas del funcionamiento del segundo ejercicio}
\end{figure}

\newpage

% Practica 3: TF-IDF
\section{Practica 3: Matriz TF-IDF}

\subsection{Introduccion}

TF-IDF (Term Frequency-Inverse Document Frequency) es una tecnica de ponderacion de terminos que evalua la importancia de una palabra en un documento dentro de una coleccion de documentos. La medida combina:

\begin{itemize}
    \item \textbf{TF (Term Frequency)}: Frecuencia de un termino en un documento especifico
    \item \textbf{IDF (Inverse Document Frequency)}: Inverso de la frecuencia del termino en toda la coleccion
\end{itemize}

La formula utilizada es:
\begin{equation}
TF\text{-}IDF(t,d) = TF(t,d) \times IDF(t)
\end{equation}

Donde:
\begin{equation}
IDF(t) = \log\left(\frac{N}{1 + df(t)}\right)
\end{equation}

\subsection{Diagrama de Flujo}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{img/diagrama3.jpg}
    \caption{Diagrama de flujo del cálculo de TF-IDF}
    \label{fig:diagrama_tfidf}
\end{figure}

\subsection{Código Fuente}

\begin{lstlisting}[caption=Clase TF-IDF]
import pandas as pd
from math import log

class TF_IDF(Tokenizer):
    """ Class for creating the TF-IDF matrix """
    
    """ Constructor """
    def __init__(self, docs:list):
        # Initialize the parent Tokenizer class
        super().__init__() 
        
        self.documents = docs
        self.tokens = []
        self.vocabulary = set()
        
        # Tokenize each document and build vocabulary
        for doc in self.documents:
            doc_tokens = self.tokenize(doc)
            self.tokens.append(doc_tokens)
            self.vocabulary.update(doc_tokens)

        # Convert vocabulary to sorted list for consistent column order
        self.vocabulary = sorted(list(self.vocabulary))

    """ Methods """
    # Compute term frequency for a given token list
    def compute_tf(self, token_list: list) -> pd.Series:
        # Create a Series with vocabulary as index, initialized to 0
        tf = pd.Series(0, index=self.vocabulary)
        
        # Count occurrences of each word
        for word in token_list:
            if word in tf.index:
                tf[word] += 1
        
        return tf
    
    # Compute inverse document frequency for the entire corpus
    def compute_idf(self) -> pd.Series:
        N = len(self.documents)
        idf = pd.Series(0.0, index=self.vocabulary)
        
        for word in self.vocabulary:
            # Count how many documents contain this word
            doc_count = sum(1 for doc_tokens in self.tokens if word in doc_tokens)
            # Calculate IDF using the smoothed formula: log(N / (1 + doc_count))
            idf[word] = log(N / (1 + doc_count))
        
        return idf

    # Compute the TF-IDF matrix
    def compute_tf_idf(self):
        # Compute TF for each document
        tf_matrix = []
        for i, doc_tokens in enumerate(self.tokens):
            tf_series = self.compute_tf(doc_tokens)
            tf_matrix.append(tf_series)
        
        # Create TF DataFrame
        tf_df = pd.DataFrame(tf_matrix, index=[f"Doc_{i+1}" for i in range(len(self.documents))])
        
        # Compute IDF
        idf_series = self.compute_idf()
        
        # Compute TF-IDF by multiplying TF matrix with IDF vector
        tf_idf_matrix = tf_df.multiply(idf_series, axis=1)
        
        return tf_idf_matrix
\end{lstlisting}

\subsection{Documentos de Prueba}

Para esta práctica se utilizaron tres documentos sobre SpongeBob y su trabajo en el Krusty Krab:

\begin{itemize}
    \item \textbf{Documento 1}: Enfoque en la pasión por el trabajo (192 palabras)
    \item \textbf{Documento 2}: Enfoque en las relaciones laborales (201 palabras)
    \item \textbf{Documento 3}: Enfoque en el arte culinario (227 palabras)
\end{itemize}

\subsection{Capturas del Funcionamiento}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{img/funcionamiento_3.png}
    \caption{Matriz TF-IDF resultante}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_3_1.png}
        \caption{Análisis de términos representativos}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\textwidth]{img/funcionamiento_3_12.png}
        \caption{Términos más significativos}
    \end{subfigure}
    \caption{Capturas del funcionamiento del tercer ejercicio}
\end{figure}

\newpage

% Conclusiones
\section{Conclusiones}

A lo largo de este proyecto, hemos logrado desarrollar e implementar tres algoritmos fundamentales del Procesamiento de Lenguaje Natural, abarcando desde la tokenización básica hasta técnicas más avanzadas como el cálculo de TF-IDF. Este trabajo nos ha permitido comprender de manera profunda cómo funcionan las bases del análisis de texto y la importancia de cada etapa en el procesamiento de información lingüística.

La implementación de estos algoritmos en Python y C++ nos ha dado la oportunidad de comparar diferentes enfoques de programación y observar cómo cada lenguaje ofrece ventajas particulares. Mientras que Python nos brindó una sintaxis clara y bibliotecas poderosas para el manejo de datos, C++ nos permitió explorar aspectos de optimización y eficiencia en el procesamiento. Ambas implementaciones fueron probadas con datos reales, lo que nos ayudó a validar la correctitud de nuestros algoritmos y a identificar áreas de mejora.

Durante el desarrollo, nos quedó claro que la tokenización es mucho más que simplemente dividir texto en palabras. Es el fundamento sobre el cual se construyen todas las demás operaciones de PLN, y su correcta implementación determina en gran medida la calidad de los resultados finales. El preprocesamiento, por su parte, demostró ser una etapa crucial para limpiar y normalizar el texto, eliminando ruido innecesario que podría afectar el análisis posterior. La conversión a minúsculas y la eliminación de stopwords redujeron significativamente el vocabulario sin perder información relevante, lo que mejoró tanto la eficiencia como la precisión de nuestros algoritmos.

El trabajo con TF-IDF nos reveló cómo es posible identificar automáticamente los términos más importantes dentro de una colección de documentos. Esta técnica nos permitió ver que no todas las palabras tienen el mismo peso informativo, y que aquellas que aparecen frecuentemente en un documento específico pero raramente en otros son las que realmente caracterizan y diferencian cada texto. Los resultados obtenidos con nuestros documentos de prueba sobre SpongeBob demostraron claramente cómo el algoritmo puede capturar los conceptos distintivos de cada documento.

Finalmente, este proyecto nos ha enseñado que la implementación eficiente no es solo una cuestión de elegir el algoritmo correcto, sino también de considerar aspectos como el manejo de memoria, la complejidad computacional y la escalabilidad. Estas prácticas nos han preparado mejor para enfrentar problemas más complejos de PLN en el futuro, proporcionándonos una base sólida tanto en los aspectos teóricos como en la implementación práctica de soluciones de procesamiento de lenguaje natural.

\newpage

% Bibliografia
\section{Bibliografia}

\begin{thebibliography}{9}

\bibitem{manning2008}
Manning, C. D., Raghavan, P., \& Schütze, H. (2008). 
\textit{Introduction to information retrieval}. 
Cambridge University Press.

\bibitem{jurafsky2019}
Jurafsky, D., \& Martin, J. H. (2019). 
\textit{Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition} (3rd ed.). 
Pearson.

\bibitem{nltk2009}
Bird, S., Klein, E., \& Loper, E. (2009). 
\textit{Natural language processing with Python: analyzing text with the natural language toolkit}. 
O'Reilly Media.

\bibitem{sklearn2011}
Pedregosa, F., et al. (2011). 
Scikit-learn: Machine learning in Python. 
\textit{Journal of machine learning research}, 12, 2825-2830.

\bibitem{pandas2010}
McKinney, W. (2010). 
Data structures for statistical computing in Python. 
\textit{Proceedings of the 9th Python in Science Conference}, 51-56.

\bibitem{cpp2011}
ISO/IEC 14882:2011. (2011). 
\textit{Information technology — Programming languages — C++}. 
International Organization for Standardization.

\end{thebibliography}

\end{document}